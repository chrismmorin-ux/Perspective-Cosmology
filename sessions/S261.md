# Session S261: LLM Challenge v3 â€” Numerical Formula Discovery

**Date**: 2026-02-07
**Focus**: Design and run LLM Challenge v3 testing numerical formula discovery
**Topic**: Meta / framework assessment
**Status**: COMPLETE
**Previous**: S257 (Red Team v2.0 + LLM Challenge v2), S259 (assumption landscape)

---

## Outcome

**V3 prompt designed and tested on GPT-4o**. Three-phase Q1 structure separates forward algebraic reasoning (Part A) from reverse-engineering (Part C). Result: **INTERESTING FAILURE** -- GPT-4o explored 13 forward candidates for alpha correction, none near 0.036. Never computed n_c^2-n_c+1=111. Never tried 28/121 for Weinberg angle despite both numbers in prompt. Axiom attempt hallucinated numerical match (claimed 137.036, actual 137.364 -- SymPy verified). V3 is informative but not decisive due to LLM's poor combinatorial search quality.

---

## Key Findings

1. **[ASSESSMENT] Q1 Alpha: INTERESTING FAILURE.** Forward search produced 13 candidates (1/44 closest at 0.023). Reverse search found 4/121 but NOT 4/111. The cyclotomic polynomial n_c^2-n_c+1=111 never appears in any LLM search path. The number 111 is not in the "natural search space" of current LLMs.

2. **[ASSESSMENT] Q2 Weinberg: SURPRISING FAILURE.** LLM never tried 28/121 = 0.2314 despite dim(Gr(4,11))=28 and dim(End(R^11))=121 being explicitly given. The most natural geometric ratio was sitting right there. Suggests LLMs are generally poor at combinatorial ratio-searching.

3. **[ASSESSMENT] Hallucination caught.** LLM's "axiom C6" claimed 137x(1+4/1507)=137.036. SymPy verification: actual value is 1511/11 = **137.3636...**. Confused additive correction 0.036 with multiplicative correction 0.00265.

4. **[ASSESSMENT] n_d/n_c swap.** Despite prompt clearly giving n_d=4, n_c=11, LLM re-derived from 137=n_d^2+n_c^2 and assigned (n_d,n_c)=(11,4) -- backwards. Poor prompt compliance.

5. **[ASSESSMENT] V3 does not resolve derivation vs discovery.** The LLM's failure to find even 28/121 (the "easy" formula) means its failure on 4/111 (the "hard" formula) is less diagnostic than hoped. Two interpretations remain: (a) formulas are post-hoc fits, (b) formulas require algebraic insight beyond brute-force search.

---

## Scripts

| Script | Tests | Status |
|--------|-------|--------|
| (no verification scripts -- assessment session) | -- | -- |

---

## Files Created/Modified

| File | Action |
|------|--------|
| `registry/llm_challenge/prompts/variant_3_numerical_discovery.md` | CREATED |
| `registry/llm_challenge/results/v3_test1_numerical_discovery.md` | CREATED |
| `registry/llm_challenge/results/SUMMARY.md` | UPDATED (V3 section + implications) |

---

## Open Questions

1. **V3 on more models**: Run on Claude (fresh) and Gemini to check if failure is universal.
2. **V3.1 design**: Give sector decomposition as established, ask specifically about polynomial evaluations at n_c. Tests whether cyclotomic connection is natural given more structure.
3. **V3.2 design**: Give formula 4/111, ask if algebraically natural. Tests understanding, not discovery.
4. **Human expert review** (PERSISTENT): Still the highest-ROI action for overall framework assessment.

---

## Key Context for Next Session

V3 is now documented with prompt file and first test result. The three-phase Q1 design (forward/compare/reverse) is the right methodology. GPT-4o failed on both the hard formula (4/111) and the easy one (28/121), which means V3 alone can't distinguish "formulas aren't natural" from "LLMs are bad at combinatorial search." The user plans to try another method for alpha in a separate session.
