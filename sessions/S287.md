# Session S287: LLM Hallucination Audit

**Date**: 2026-02-07
**Focus**: Comprehensive quality audit — LLM hallucination patterns, phantom scripts, script health, THEOREM tags
**Status**: COMPLETE
**Previous**: S264 (Quality Engine Run #5 fixes)

---

## Outcome

Systematic 5-stream audit across the entire framework. Found **no outright fabricated results**, but identified **1 CRITICAL** and **6 HIGH** hallucination-adjacent patterns. Framework's self-awareness (HRS, Red Team, falsification tracking) is remarkably strong. Main risks are precision inflation and post-hoc fitting disguised as prediction.

---

## Key Findings

### Stream 1: Phantom Script References
**8 phantoms out of ~260 references (3%)**

1. **[HIGH] `h_gamma_derivation.py`** — Referenced in `core/18_dynamics.md` as verification. Script does not exist. False verification claim on a core document.
2. **[MEDIUM]** 4 stale renames or never-created scripts in investigation files
3. **[LOW]** 3 explicitly acknowledged as planned/not-yet-created

### Stream 2: Script Execution Health
**651 scripts total**

| Metric | Count | Percentage |
|--------|-------|-----------|
| PASS | 579 | 88.9% |
| FAIL | 46 | 7.1% |
| ERROR | 25 | 3.8% |
| Timeout | 1 | 0.2% |

- **6,825 assertions**: 6,744 PASS (98.81%), 81 FAIL (1.19%)
- **23 of 25 errors** are Windows cp1252 Unicode encoding issues (not real failures)
- Adjusting for Unicode: effective pass rate ~92.5%
- Notable failures: `band_B_coefficient_analysis.py` (4), `division_algebra_ambiguity_analysis.py` (9), `w_z_mass_from_v.py` (3), `weinberg_coefficient_origin.py` (3)

### Stream 3: Hallucination Warning Patterns
**15 findings total (1 CRITICAL, 6 HIGH, 5 MEDIUM, 3 LOW)**

- **[CRITICAL] HP-005: Weinberg "0.00 sigma" with hidden free parameter** — S276 claims sin^2(theta_W) dressed formula achieves "0.00 sigma" from experiment. But C_W = 1/(4*pi) was *extracted from data*, not predicted. This is a 1-parameter fit to 1 data point = guaranteed perfect match. Not a prediction.
- **[HIGH] HP-006: SM imports labeled [DERIVATION]** — Several places where SM facts (e.g., "3 generations") are treated as framework derivations when they actually depend on [A-IMPORT] values.
- **[HIGH] HP-007: "EXACT" inflation** — Pattern of calling approximate matches "EXACT" (e.g., "b_0=11=n_c [EXACT IDENTITY]" when b_0 coefficient formula involves SM particle content, not derived).
- **[HIGH] HP-008: C=24/11 post-hoc discovery** — Coefficient was found by searching what C value gives best fit, then justified. Framework documents acknowledge this but session summaries sometimes omit the post-hoc nature.
- **[HIGH] HP-009: F=C circularity** — Some derivation chains use F=C (axiom) to derive results that were used to motivate F=C. Not circular if axiom status is maintained, but language sometimes implies F=C was derived.
- **[HIGH] HP-010: eps*=alpha^2 circular derivation** — Alpha radiative gap analysis defines eps* from the gap, then shows eps* ~ alpha^2, which is tautological since alpha defines the gap.
- **[HIGH] HP-011: CODATA version inconsistency** — Most alpha scripts use CODATA 2018 (1/alpha = 137.035999084), but some reference CODATA 2022 (137.035999206). The "0.0002 ppm" claim for C=24/11 is valid only against CODATA 2018; against 2022, gap is ~0.001 ppm (~5x worse but still excellent).

**5 systemic patterns identified:**
1. "EXACT" label inflation (approximate -> exact)
2. Identification masquerading as derivation (matching a number vs deriving it)
3. Post-hoc fitting with prediction language
4. Free parameters disguised as "structural"
5. Correlated claims treated as independent evidence

### Stream 4: Numerical Cross-Checks
- **Alpha 1/alpha = 137 + 4/111**: Arithmetic CORRECT. 0.27 ppm CORRECT (against CODATA 2018).
- **Weinberg 123/532**: SUPERSEDED formula still in active directory. Current framework uses 28/121 tree-level. Old script `weinberg_best_formula.py` should be archived.
- **C=24/11 coefficient**: Internally consistent. But C was extracted, not predicted [A-IMPORT]. Well-documented in investigation file.
- **1-+ glueball "0.5%"**: This is ratio-only precision (m_{1+-}/m_{0++} = 1.625 vs lattice 1.618). Absolute mass error is 4.6%. Session summaries sometimes say "0.5% from lattice" without qualification.

### Stream 5: [THEOREM] Tag Audit
**~260 [THEOREM] occurrences across framework**

- **~75% correctly tagged**: Rigorous proofs from stated assumptions
- **~20% should downgrade to [DERIVATION]**: These inherit dependencies on DERIVATION-level results (e.g., THM_0487) or have acknowledged gaps
- **~5% needs review**: Computational patterns called [THEOREM] (existence proofs by exhaustion are legitimate, but some are just "we checked N cases")
- **0% depends on [CONJECTURE]**: No theorem tags found depending on unproven conjectures
- **0 hallucination indicators**: No fabricated proofs detected

Key downgrades recommended:
- THM_04B2 parts b/c (perspective rank selection — depend on DERIVATION-level inputs)
- Non-observations A1-A4 (depend on THM_0487 which is [DERIVATION])
- Some entanglement findings (depend on THM_0494, also [DERIVATION])

---

## Overall Assessment

| Category | Grade | Notes |
|----------|-------|-------|
| Script health | B+ | 92.5% effective pass rate (excluding Unicode issues) |
| Phantom references | A- | 3% phantom rate, only 1 HIGH |
| Confidence tagging | B | 75% correct THEOREM tags, 20% need downgrade |
| Numerical integrity | A | Core formulas verified correct |
| Hallucination defense | A | Framework's own HRS/Red Team system is effective |
| Precision honesty | C+ | "EXACT" inflation, CODATA inconsistency, ratio/absolute conflation |
| Post-hoc transparency | B- | Investigation files are transparent; session summaries less so |

**Bottom line**: The framework is unusually self-aware about its limitations. The main risk is not outright hallucination but **precision inflation** — legitimate results being presented more impressively than warranted. The CODATA inconsistency and "0.00 sigma" with a free parameter are the most actionable findings.

---

## Scripts

| Script | Tests | Status | Notes |
|--------|-------|--------|-------|
| (audit — no new scripts) | — | — | Used 4 background agents + direct analysis |

---

## Files Modified

- `registry/HALLUCINATION_LOG.md` — Added HP-005 through HP-011
- `sessions/INDEX.md` — Added S287
- `registry/ACTIVE_SESSIONS.md` — Registered/deregistered S287

---

## Open Questions

1. **EQ-NEW: CODATA standardization** — Should all scripts be updated to CODATA 2022? What's the impact on precision claims?
2. **Weinberg old formula cleanup** — Archive `weinberg_best_formula.py` or rename to `weinberg_OLD_formula.py`?
3. **THEOREM downgrade campaign** — Systematic pass to downgrade the ~20% over-tagged theorems?
4. **1-+ glueball language** — Standardize "0.5% ratio error / 4.6% absolute error" in all references?
5. **HP-005 Weinberg "0.00 sigma"** — Is C_W=1/(4*pi) genuinely predicted or extracted?

---

## Key Context for Next Session

This audit found the framework's computational backbone is solid (98.8% assertion pass rate, 3% phantom scripts). The main quality risks are **language-level**: precision inflation in session summaries, CODATA version inconsistency affecting sub-ppm claims, and one critical case of a free parameter disguised as a prediction (Weinberg dressed formula). No fabricated calculations or proofs were detected. The framework's self-skepticism infrastructure (HRS, Red Team, HALLUCINATION_LOG) is genuinely effective — most issues found here are already acknowledged somewhere in the framework.
